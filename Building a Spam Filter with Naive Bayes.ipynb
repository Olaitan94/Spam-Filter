{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first task is to \"teach\" the computer how to classify messages.\n",
    "To do that, we'll use the multinomial Naive Bayes algorithm along with a dataset of 5,572 SMS messages that are already classified by humans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the SMSSpamCollection file using the read_csv() function from the pandas package.\n",
    "The data points are tab separated, so we'll need to use the sep='\\t' parameter for our read_csv() function.\n",
    "The dataset doesn't have a header row, which means we need to use the header=None parameter, otherwise the first row will be wrongly used as the header row.\n",
    "Use the names=['Label', 'SMS'] parameter to name the columns as Label and SMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     0.865937\n",
      "spam    0.134063\n",
      "Name: Label, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "messages = pd.read_csv('SMSSpamCollection', sep = '\\t', header = None, names = ['Label', 'SMS'])\n",
    "\n",
    "#print(messages.info())\n",
    "#print(messages.shape)\n",
    "print(messages['Label'].value_counts(normalize = True))\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the spam filter is done, we'll need to test how good it is with classifying new messages. To test the spam filter,  first split the dataset into two categories:\n",
    "A training set, which we'll use to \"train\" the computer how to classify messages.\n",
    "A test set, which we'll use to test how good the spam filter is with classifying new messages.\n",
    "We're going to keep 80% of our dataset for training, and 20% for testing (we want to train the algorithm on as much data as possible, but we also want to have enough test data). The dataset has 5,572 messages, which means that:\n",
    "\n",
    "The training set will have 4,458 messages (about 80% of the dataset).\n",
    "The test set will have 1,114 messages (about 20% of the dataset).\n",
    "\n",
    "To better understand the purpose of putting a test set aside, let's begin by observing that all 1,114 messages in our test set are already classified by a human. When the spam filter is ready, we're going to treat these messages as new and have the filter classify them. Once we have the results, we'll be able to compare the algorithm classification with that done by a human, and this way we'll see how good the spam filter really is.\n",
    "The goal is to create a spam filter that classifies new messages with an accuracy greater than 80% — so we expect that more than 80% of the new messages will be classified correctly as spam or ham (non-spam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To  create a training and a test set, We're going to start by randomizing the entire dataset to ensure that spam and ham messages are spread properly throughout the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.86541\n",
       "spam    0.13459\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#randomize the entire dataset using dataframe.sample()\n",
    "#Use the frac=1 parameter to randomize the entire dataset.\n",
    "#Use the random_state=1 parameter to make sure your results are reproducible.\n",
    "\n",
    "random_message = messages.sample(frac=1, random_state = 1)\n",
    "\n",
    "# Calculate index for split\n",
    "training_test_index = round(len(random_message) * 0.8)\n",
    "\n",
    "# Training/Test split\n",
    "#Reset the index labels for both data sets — the index labels remained unordered after randomization. \n",
    "#You can use the DataFrame.reset_index() method.\n",
    "training_set = random_message[:training_test_index].reset_index(drop=True)\n",
    "test_set = random_message[training_test_index:].reset_index(drop=True)\n",
    "\n",
    "#Find the percentage of spam and ham in both the training and the test set.\n",
    "training_set['Label'].value_counts(normalize = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.868043\n",
       "spam    0.131957\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set['Label'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning\n",
    "The SMS column doesn't exist anymore.\n",
    "Instead, the SMS column is replaced by a series of new columns, where each column represents a unique word from the vocabulary.\n",
    "Each row describes a single message. For instance, the first row corresponds to the message \"SECRET PRIZE! CLAIM SECRET PRIZE NOW!!\", and it has the values spam, 2, 2, 1, 1, 0, 0, 0, 0, 0. These values tell us that:\n",
    "The message is spam.\n",
    "The word \"secret\" occurs two times inside the message.\n",
    "The word \"prize\" occurs two times inside the message.\n",
    "The word \"claim\" occurs one time inside the message.\n",
    "The word \"now\" occurs one time inside the message.\n",
    "The words \"coming\", \"to\", \"my\", \"party\", and \"winner\" occur zero times inside the message.\n",
    "All words in the vocabulary are in lower case, so \"SECRET\" and \"secret\" come to be considered to be the same word.\n",
    "Punctuation is not taken into account anymore (for instance, we can't look at the table and conclude that the first message initially had three exclamation marks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin the data cleaning process by removing the punctuation and bringing all the words to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['SMS'] = training_set['SMS'].str.replace('\\W', ' ')  # remove all non word character\n",
    "training_set['SMS'] = training_set['SMS'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's create a list with all of the unique words that occur in the messages of our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7783\n"
     ]
    }
   ],
   "source": [
    "training_set['SMS'] = training_set['SMS'].str.split()\n",
    "vocabulary = []\n",
    "for row in training_set['SMS']:\n",
    "    for word in row:\n",
    "        vocabulary.append(word)\n",
    "vocabulary = set(vocabulary)\n",
    "vocabulary = list(vocabulary) #This is the list of unique words\n",
    "print(len(vocabulary))\n",
    "\n",
    "#Now create a frequency table in a dictinoary for the unique words\n",
    "word_count_per_sms = { unique_word : [0] * len(training_set['SMS']) for unique_word in vocabulary}\n",
    "for index,sms in enumerate(training_set['SMS']):\n",
    "        for word in sms:\n",
    "            word_count_per_sms[word][index] += 1\n",
    "\n",
    "# i = 0\n",
    "# for key, value in word_count_per_sms.items():\n",
    "#     print(key,value)\n",
    "#     print('\\n')\n",
    "#     i += 1\n",
    "#     if i == 20:\n",
    "#         break\n",
    "\n",
    "#convert dictionary to a dtaframe\n",
    "word_count_per_sms = pd.DataFrame(word_count_per_sms)\n",
    "# word_count_per_sms.info()\n",
    "# word_count_per_sms.head()\n",
    "\n",
    "#join the original dataframe (the one with the label and sms columns) with this new word_count dataframe\n",
    "final_training_set = pd.concat([training_set,word_count_per_sms], axis = 1)\n",
    "#final_training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a start, let's first calculate:\n",
    "\n",
    "P(Spam) - probability of spam\n",
    "P(Ham) - probability of non spam\n",
    "NSpam - Total no of words in all the spam messages\n",
    "NHam - Total no of words in all the non spam messages\n",
    "NVocabulary - Total no of words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15190\n",
      "57237\n",
      "7783\n"
     ]
    }
   ],
   "source": [
    "#the final training set data will be divided into two different data sets, spam and ham\n",
    "training_set_spam = final_training_set[final_training_set['Label'] == 'spam'].reset_index(drop = True)\n",
    "training_set_ham = final_training_set[final_training_set['Label'] == 'ham'].reset_index(drop=True)\n",
    "\n",
    "#calculate P(spam) & P(non_spam)\n",
    "p_spam, p_ham = final_training_set['Label'].value_counts(normalize=True)\n",
    "#print(p_spam, p_ham)\n",
    "\n",
    "#calculate n_spam\n",
    "total_word_count_per_spam_message = training_set_spam['SMS'].apply(len)\n",
    "n_spam = total_word_count_per_spam_message.sum()\n",
    "print(n_spam)\n",
    "\n",
    "#calculate n_ham\n",
    "total_word_count_per_ham_message = training_set_ham['SMS'].apply(len)\n",
    "n_ham = total_word_count_per_ham_message.sum()\n",
    "print(n_ham)\n",
    "\n",
    "# N_Vocabulary\n",
    "n_vocabulary = len(vocabulary)\n",
    "print(n_vocabulary)\n",
    "\n",
    "# Laplace smoothing\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the constant terms calculated above, we can move on with calculating the parameters P(w/spam) and P(w/ham)\n",
    "Each parameter will thus be a conditional probability value associated with each word in the vocabulary.\n",
    "\n",
    "The parameters are calculated using the formulas:\n",
    "P(w/spam) = (N(w/spam) + alpha)/(n_spam + alpha*n_vocab))\n",
    "P(w/ham) = (N(w/ham) + alpha)/(n_ham + alpha*n_vocab))\n",
    "\n",
    "we can use our training set to calculate the probability for each word in our vocabulary. If our vocabulary contained only the words \"lost\", \"navigate\", and \"sea\", then we'd need to calculate six probabilities:\n",
    "\n",
    "P(\"lost\"|Spam) and P(\"lost\"|Ham)\n",
    "P(\"navigate\"|Spam) and P(\"navigate\"|Ham)\n",
    "P(\"sea\"|Spam) and P(\"sea\"|Ham)\n",
    "\n",
    "We have 7,783 words in our vocabulary, which means we'll need to calculate a total of 15,566 probabilities. For each word, we need to calculate both P(wi|Spam) and P(wi|Ham).\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets calculate P(w/spam) & P(w/ham) for the unique words in the vocabulary\n",
    "#we will have two dictionaries, one for spam probalities and the other for ham\n",
    "\n",
    "spam_prob_dict ={}\n",
    "for word in vocabulary:\n",
    "    n_word_spam = training_set_spam[word].sum()\n",
    "    spam_prob_dict[word] = (n_word_spam + alpha)/(n_spam + (alpha * n_vocabulary))\n",
    "    \n",
    "ham_prob_dict = {}\n",
    "for word in vocabulary:\n",
    "    n_word_ham = training_set_ham[word].sum()\n",
    "    ham_prob_dict[word] = (n_word_ham + alpha)/(n_ham + (alpha * n_vocabulary))\n",
    "    \n",
    "# i = 0\n",
    "# for key, value in spam_prob_dict.items():\n",
    "#     print(key,value)\n",
    "#     print('\\n')\n",
    "#     i += 1\n",
    "#     if i == 20:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've calculated all the constants and parameters we need, we can start creating the spam filter. The spam filter can be understood as a function that:\n",
    "-- Takes a new message (w1, w2, ..., wn) as input. The message would be a string\n",
    "-- Cleans the message (removes all non word characters and also changes all the words to lower case using re.sub() and str.lower())\n",
    "-- splits all the words in the message into a string using str.split()\n",
    "-- Any new words that are not in the vocabulary of unique words will be ignored and no probability will calculated for them.\n",
    "-- Calculates P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn)\n",
    "-- Compares the values of P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn), and:\n",
    "----If P(Ham|w1, w2, ..., wn) > P(Spam|w1, w2, ..., wn), then the message is classified as ham.\n",
    "----If P(Ham|w1, w2, ..., wn) < P(Spam|w1, w2, ..., wn), then the message is classified as spam.\n",
    "----If P(Ham|w1, w2, ..., wn) = P(Spam|w1, w2, ..., wn), then the algorithm may request human help.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Label                                                SMS  \\\n",
      "9      ham                             I liked the new mobile   \n",
      "18     ham           and  picking them up from various points   \n",
      "56     ham                       what is your account number?   \n",
      "114   spam  Not heard from U4 a while. Call me now am here...   \n",
      "125    ham                           Your brother is a genius   \n",
      "152    ham                  Unlimited texts. Limited minutes.   \n",
      "159    ham                                       26th OF JULY   \n",
      "176    ham                            Your dad is back in ph?   \n",
      "182    ham                         Surely result will offer:)   \n",
      "186    ham     They r giving a second chance to rahul dengra.   \n",
      "195    ham  Hi.:)technical support.providing assistance to...   \n",
      "247    ham                          Which channel:-):-):):-).   \n",
      "249    ham                                            Okie...   \n",
      "251    ham                        Hahaha..use your brain dear   \n",
      "271    ham                                WHO ARE YOU SEEING?   \n",
      "284    ham                             Nokia phone is lovly..   \n",
      "293    ham  A Boy loved a gal. He propsd bt she didnt mind...   \n",
      "302    ham                   No calls..messages..missed calls   \n",
      "304    ham          This phone has the weirdest auto correct.   \n",
      "319    ham  We have sent JD for Customer Service cum Accou...   \n",
      "331    ham                          Just taste fish curry :-P   \n",
      "351    ham                     No. Yes please. Been swimming?   \n",
      "355    ham                                     Yup next stop.   \n",
      "359    ham                    You see the requirements please   \n",
      "367    ham   G says you never answer your texts, confirm/deny   \n",
      "369    ham             Idea will soon get converted to live:)   \n",
      "398    ham  Hasn't that been the pattern recently crap wee...   \n",
      "411    ham          Every monday..nxt week vl be completing..   \n",
      "489    ham                   Nan sonathaya soladha. Why boss?   \n",
      "492    ham  Madam,regret disturbance.might receive a refer...   \n",
      "504   spam  Oh my god! I've found your number again! I'm s...   \n",
      "546   spam  Hi babe its Chloe, how r u? I was smashed on s...   \n",
      "584    ham                                               Okie   \n",
      "605    ham  staff.science.nus.edu.sg/~phyhcmk/teaching/pc1323   \n",
      "664    ham                                              G.W.R   \n",
      "667    ham  Garbage bags, eggs, jam, bread, hannaford whea...   \n",
      "689    ham            Networking technical support associate.   \n",
      "706    ham                           Gibbs unsold.mike hussey   \n",
      "710    ham                          Ok... Ur typical reply...   \n",
      "720    ham                      Alright i have a new goal now   \n",
      "723    ham             Except theres a chick with huge boobs.   \n",
      "737    ham             Also are you bringing galileo or dobby   \n",
      "741   spam  0A$NETWORKS allow companies to bill for SMS, s...   \n",
      "824    ham        These won't do. Have to move on to morphine   \n",
      "827    ham                             Send me your resume:-)   \n",
      "909    ham                      Whom you waited for yesterday   \n",
      "933    ham  Do you hide anythiing or keeping distance from me   \n",
      "953   spam  Hello. We need some posh birds and chaps to us...   \n",
      "983    ham                 Raviyog Peripherals bhayandar east   \n",
      "999    ham                           You unbelievable faglord   \n",
      "1063   ham                                                645   \n",
      "1073   ham  I (Career Tel) have added u as a contact on IN...   \n",
      "1093   ham                         Prepare to be pleasured :)   \n",
      "\n",
      "                                      predicted  Accuracy  \n",
      "9                                          spam     False  \n",
      "18                                         spam     False  \n",
      "56                                         spam     False  \n",
      "114                                         ham     False  \n",
      "125                                        spam     False  \n",
      "152                                        spam     False  \n",
      "159                                        spam     False  \n",
      "176                                        spam     False  \n",
      "182                                        spam     False  \n",
      "186                                        spam     False  \n",
      "195                                        spam     False  \n",
      "247                                        spam     False  \n",
      "249                                        spam     False  \n",
      "251                                        spam     False  \n",
      "271                                        spam     False  \n",
      "284                                        spam     False  \n",
      "293   Same probability, needs human to classify     False  \n",
      "302                                        spam     False  \n",
      "304                                        spam     False  \n",
      "319                                        spam     False  \n",
      "331                                        spam     False  \n",
      "351                                        spam     False  \n",
      "355                                        spam     False  \n",
      "359                                        spam     False  \n",
      "367                                        spam     False  \n",
      "369                                        spam     False  \n",
      "398                                        spam     False  \n",
      "411                                        spam     False  \n",
      "489                                        spam     False  \n",
      "492                                        spam     False  \n",
      "504                                         ham     False  \n",
      "546                                         ham     False  \n",
      "584                                        spam     False  \n",
      "605                                        spam     False  \n",
      "664                                        spam     False  \n",
      "667                                        spam     False  \n",
      "689                                        spam     False  \n",
      "706                                        spam     False  \n",
      "710                                        spam     False  \n",
      "720                                        spam     False  \n",
      "723                                        spam     False  \n",
      "737                                        spam     False  \n",
      "741                                         ham     False  \n",
      "824                                        spam     False  \n",
      "827                                        spam     False  \n",
      "909                                        spam     False  \n",
      "933                                        spam     False  \n",
      "953                                         ham     False  \n",
      "983                                        spam     False  \n",
      "999                                        spam     False  \n",
      "1063                                       spam     False  \n",
      "1073                                       spam     False  \n",
      "1093                                       spam     False  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def classify(message):\n",
    "    msg = re.sub('\\W', ' ', message)\n",
    "    msg = msg.lower()\n",
    "    msg_list = msg.split()\n",
    "    p_spam_list = []  #list to store the probability of all the words for spam in the message \n",
    "    p_ham_list = []   #list to store the probability of all the words for ham in the message\n",
    "    for word in msg_list:\n",
    "        if word in vocabulary:  #confirm the word is in our vocab\n",
    "            p_spam_list.append(spam_prob_dict[word]) #put the probability for the word in the spam list \n",
    "            p_ham_list.append(ham_prob_dict[word])  #put the probability for the word in the ham list\n",
    "    #find p(spam given message\n",
    "    p_spam_message = p_spam * np.prod(p_spam_list)\n",
    "    p_ham_message = p_ham * np.prod(p_ham_list)\n",
    "    classification = None\n",
    "    if p_spam_message == p_ham_message:\n",
    "        classification = 'Same probability, needs human to classify'\n",
    "    elif p_spam_message > p_ham_message:\n",
    "        classification = 'spam'\n",
    "    else:\n",
    "        classification = 'ham'\n",
    "    return classification\n",
    "\n",
    "\n",
    "#print(classify('WINNER!! This is the secret code to unlock the money: C3421.'))\n",
    "#print(classify(\"Sounds good, Tom, then see u there\"))\n",
    "\n",
    "#now we will use the test data set to find the accuracy of the spam filter\n",
    "test_set['predicted'] = test_set['SMS'].apply(classify)\n",
    "\n",
    "#print(test_set.head())\n",
    "\n",
    "#now we will create another column for the accuracy of the prediction (i.e whether true or false)\n",
    "\n",
    "test_set['Accuracy'] = test_set['Label'] == test_set['predicted']\n",
    "\n",
    "#now we will check the accuracy\n",
    "#print(test_set['Accuracy'].value_counts(normalize = True) * 100)\n",
    "\n",
    "print (test_set[test_set['Accuracy'] == False])\n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
